# Project Coding Principles

## Data Integrity

- Never truncate data arbitrarily (no `[:100]` or `[:2000]` slicing)
- Preserve full information; let consumers handle their own limits

## No Dead Code

- Use everything you create: imports, variables, clones, instantiations
- Remove unused code rather than leaving it dormant

## Smart Context Management

- Prefer structured data (test reports, APIs) over raw logs
- When raw data is necessary, extract relevant content (errors, failures, warnings) instead of full dumps

## Parallel Execution

- Run independent, stateless operations in parallel
- Handle failures gracefully: one failure should not crash all parallel tasks
- Capture exceptions and continue processing

## File Handling

- Preserve user edits when modifying files
- Add missing elements rather than replacing entire content
- Never overwrite user customizations

## Communication

- Explain data flow through the system, not just variable locations
- Show how components connect and interact

## Architecture

### CLI-Based AI Integration

This project uses AI CLI tools (Claude CLI, Gemini CLI, Cursor Agent CLI) instead of direct SDK integrations:

- **No SDK dependencies**: AI providers are called via subprocess
- **Provider-agnostic**: Easy to add new AI CLIs (see README)
- **Auth handled externally**: CLIs manage their own authentication
- **Environment-driven**: `AI_PROVIDER` env var selects the provider (`claude`, `gemini`, or `cursor`)

### Key Components

| Component | Purpose |
|-----------|---------|
| `call_ai_cli()` | Single function for all AI CLI calls |
| `get_failure_signature()` | Deduplicates identical test failures |
| `analyze_failure_group()` | Analyzes unique failures, applies to all matches |
| `run_parallel_with_limit()` | Bounded parallel execution |
| `JiraClient` | Searches Jira for matching bugs (Cloud + Server/DC) |
| `enrich_with_jira_matches()` | Post-processing: attaches Jira matches to PRODUCT BUG failures |
| `_filter_matches_with_ai()` | AI-powered relevance filtering of Jira candidates |

### Failure Deduplication

When multiple tests fail with the same error:
1. Failures are grouped by error signature (MD5 hash of error + stack trace)
2. Only one AI CLI call per unique error type
3. Analysis is applied to all failures with matching signature
4. Reduces redundant API calls and output

### Jira Integration (Optional)

When configured, searches Jira for existing bugs matching PRODUCT BUG failures:
1. AI generates specific search keywords during analysis
2. After analysis, keywords are used to search Jira (Bug type, summary search)
3. AI evaluates each Jira candidate's relevance by reading its summary and description
4. Only genuinely relevant matches are attached to the result
5. Jira errors never crash the pipeline — all failures are swallowed gracefully

### Logging

Uses `python-simple-logger`:
- INFO: Milestones (job started, AI calls, completed)
- DEBUG: Detailed operations (response lengths, extracted data)
- Configured via `LOG_LEVEL` environment variable

## API Design

### Environment Variable / Payload Parity

Every environment variable that configures the service must also be available as a per-request field in the API payload. This allows callers to override any configuration on a per-request basis without changing the server environment.

When adding a new environment variable:
1. Add the field to `Settings` in `config.py`
2. Add the corresponding request field to `BaseAnalysisRequest` (or `AnalyzeRequest` if endpoint-specific) in `models.py`
3. Add the field to `_merge_settings()` in `main.py` so request values override env defaults
4. Update the Request Override Priority table in `README.md`

Exceptions (server-level only, no payload equivalent):
- `DEBUG` — server reload toggle
- `LOG_LEVEL` — server log verbosity
- `PROMPT_FILE` — server-local file path
